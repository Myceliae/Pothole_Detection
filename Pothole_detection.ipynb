{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Myceliae/Pothole_Detection/blob/main/Pothole_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtxu9TIDC8Uw"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import kagglehub\n",
        "chitholian_annotated_potholes_dataset_path = \"/content/drive/MyDrive/157-Potholes-Compressed/annotated-potholes-dataset/versions/1/annotated-images\"\n",
        "andrewmvd_pothole_detection_path = kagglehub.dataset_download('andrewmvd/pothole-detection')\n",
        "ashishkumarak_training_setzip_path = kagglehub.dataset_download('ashishkumarak/training-setzip')\n",
        "\n",
        "print('Data source import complete.')\n",
        "\n",
        "print(\"Path to dataset files:\", chitholian_annotated_potholes_dataset_path)\n",
        "print(\"Path to dataset files:\", andrewmvd_pothole_detection_path)\n",
        "print(\"Path to dataset files:\", ashishkumarak_training_setzip_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysMVlitdC8U6"
      },
      "outputs": [],
      "source": [
        "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# # For example, here's several helpful packages to load\n",
        "\n",
        "# import numpy as np # linear algebra\n",
        "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# # Input data files are available in the read-only \"../input/\" directory\n",
        "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# import os\n",
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BSNHHqrMC8U7"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations\n",
        "!pip install timm\n",
        "!pip install --upgrade opencv-contrib-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5o0L4m_C8U8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import random\n",
        "import os\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "from xml.dom import minidom\n",
        "import csv\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtpNwdFmC8U9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "pattern = r\"(\\d+)\\.jpg$\"\n",
        "img_numbers=[]\n",
        "\n",
        "for dirname, _, filenames in os.walk(chitholian_annotated_potholes_dataset_path):\n",
        "    for filename in filenames:\n",
        "        match = re.search(pattern,filename,re.IGNORECASE)\n",
        "        if match:\n",
        "            number = match.group(1)\n",
        "            img_numbers.append(number)\n",
        "\n",
        "img_numbers.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFDySd7RC8U-"
      },
      "outputs": [],
      "source": [
        "def extract_xml_contents(annot_directory, image_dir, file_dir):\n",
        "\n",
        "        file = minidom.parse(annot_directory)\n",
        "\n",
        "        # Get the height and width for our image\n",
        "        height, width = cv2.imread(image_dir).shape[:2]\n",
        "\n",
        "        # Get the bounding box co-ordinates\n",
        "        xmin = file.getElementsByTagName('xmin')\n",
        "        x1 = int(xmin[0].firstChild.data)\n",
        "\n",
        "        ymin = file.getElementsByTagName('ymin')\n",
        "        y1 = int(ymin[0].firstChild.data)\n",
        "\n",
        "        xmax = file.getElementsByTagName('xmax')\n",
        "        x2 = int(xmax[0].firstChild.data)\n",
        "\n",
        "        ymax = file.getElementsByTagName('ymax')\n",
        "        y2 = int(ymax[0].firstChild.data)\n",
        "\n",
        "        files = file.getElementsByTagName('filename')\n",
        "        filename = files[0].firstChild.data\n",
        "        filename = os.path.join(file_dir,filename)\n",
        "\n",
        "        return filename, width, height, x1,y1,x2,y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCSsbn0zC8U_"
      },
      "source": [
        "### Dataset 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AbmemmbC8VC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to convert XML files to CSV\n",
        "def xml_to_csv(data_dir):\n",
        "\n",
        "  # List containing all our attributes regarding each image\n",
        "    xml_list = []\n",
        "\n",
        "  # Loop over each of the image and its label\n",
        "    for i in img_numbers:\n",
        "\n",
        "        mat = f\"img-{i}.xml\"\n",
        "        image_file = f\"img-{i}.jpg\"\n",
        "\n",
        "      # Full mat path\n",
        "        mat_path = os.path.join(data_dir, mat)\n",
        "\n",
        "      # Full path Image\n",
        "        img_path = os.path.join(data_dir, image_file)\n",
        "\n",
        "      # Get Attributes for each image\n",
        "        value = extract_xml_contents(mat_path, img_path,data_dir)\n",
        "\n",
        "      # Append the attributes to the mat_list\n",
        "        xml_list.append(value)\n",
        "\n",
        "  # Columns for Pandas DataFrame\n",
        "    column_name = ['filename', 'width', 'height', 'xmin', 'ymin',\n",
        "                 'xmax', 'ymax']\n",
        "\n",
        "  # Create the DataFrame from mat_list\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "\n",
        "  # Return the dataframe\n",
        "    return xml_df\n",
        "\n",
        "# Run the function to convert all the xml files to a Pandas DataFrame\n",
        "labels_df1 = xml_to_csv(data_dir=chitholian_annotated_potholes_dataset_path)\n",
        "\n",
        "# Saving the Pandas DataFrame as CSV File\n",
        "labels_df1.to_csv(('dataset.csv'), index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zJg7cTFC8VE"
      },
      "outputs": [],
      "source": [
        "labels_df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l59iQGgtC8VE"
      },
      "source": [
        "### Dataset 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g27XWVc7C8VF"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file and rename the columns\n",
        "labels_df2 = pd.read_csv(\"/kaggle/input/training-setzip/train/labels.csv\")\n",
        "labels_df2.columns = ['filename', 'LabelName', 'xmin', 'xmax', 'ymin', 'ymax']\n",
        "labels_df2.drop(\"LabelName\", axis=1, inplace=True)\n",
        "\n",
        "# Add a new column with the image filename and path\n",
        "labels_df2['filename'] = labels_df2['filename'].apply(lambda x: \"/kaggle/input/training-setzip/train/images/\" + x)\n",
        "\n",
        "# Add new columns with the image height and width\n",
        "heights, widths = [], []\n",
        "for _, row in labels_df2.iterrows():\n",
        "    height, width = cv2.imread(row['filename']).shape[:2]\n",
        "    heights.append(height)\n",
        "    widths.append(width)\n",
        "\n",
        "labels_df2 = labels_df2.assign(height=heights, width=widths)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "labels_df2"
      ]
    },
    {
      "source": [
        "# Read the CSV file and rename the columns\n",
        "# Use the path obtained from kagglehub.dataset_download instead of the Kaggle-specific path\n",
        "labels_df2 = pd.read_csv(os.path.join(ashishkumarak_training_setzip_path, \"train/labels.csv\"))\n",
        "labels_df2.columns = ['filename', 'LabelName', 'xmin', 'xmax', 'ymin', 'ymax']\n",
        "labels_df2.drop(\"LabelName\", axis=1, inplace=True)\n",
        "\n",
        "# Add a new column with the image filename and path\n",
        "# Update the image path to be relative to the downloaded dataset path\n",
        "labels_df2['filename'] = labels_df2['filename'].apply(lambda x: os.path.join(ashishkumarak_training_setzip_path, \"train/images/\", x))\n",
        "\n",
        "# Add new columns with the image height and width\n",
        "heights, widths = [], []\n",
        "for _, row in labels_df2.iterrows():\n",
        "    # Ensure the image file exists before trying to read it\n",
        "    img_full_path = row['filename']\n",
        "    if os.path.exists(img_full_path):\n",
        "        height, width = cv2.imread(img_full_path).shape[:2]\n",
        "        heights.append(height)\n",
        "        widths.append(width)\n",
        "    else:\n",
        "        # Handle cases where the image file might be missing (optional)\n",
        "        print(f\"Warning: Image file not found at {img_full_path}\")\n",
        "        heights.append(None) # Or some other placeholder\n",
        "        widths.append(None) # Or some other placeholder\n",
        "\n",
        "\n",
        "labels_df2 = labels_df2.assign(height=heights, width=widths)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "labels_df2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LmjJ7DFlEBKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHL1eic4C8VF"
      },
      "outputs": [],
      "source": [
        "labels_df = pd.concat([labels_df1,labels_df2],axis=0)\n",
        "labels_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7jGFhIPC8VG"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = chitholian_annotated_potholes_dataset_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llEESHEvC8VH"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "IMG_SIZE = 128\n",
        "LR = 0.01\n",
        "EPOCHS = 30\n",
        "MODEL_NAME = \"res2net50d.in1k\"\n",
        "NUM_COR = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D41WvVhQC8VH"
      },
      "outputs": [],
      "source": [
        "row = labels_df.iloc[12]\n",
        "\n",
        "img = cv2.imread(row.filename)\n",
        "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "pt1 = (row.xmin, row.ymin)\n",
        "pt2 = (row.xmax, row.ymax)\n",
        "\n",
        "bnd_box_img = cv2.rectangle(img, pt1, pt2, (255,0,0),1)\n",
        "\n",
        "plt.imshow(bnd_box_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TYh0kR3C8VI"
      },
      "outputs": [],
      "source": [
        "train_df, valid_df = train_test_split(labels_df1,test_size=0.10,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToVOIMpYC8VI"
      },
      "outputs": [],
      "source": [
        "import albumentations as A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTxVRCj6C8VJ"
      },
      "outputs": [],
      "source": [
        "train_augs = A.Compose([\n",
        "    A.Resize(IMG_SIZE,IMG_SIZE),\n",
        "], bbox_params=A.BboxParams(format=\"pascal_voc\",label_fields=[\"class_labels\"]))\n",
        "\n",
        "valid_augs = A.Compose([\n",
        "    A.Resize(IMG_SIZE,IMG_SIZE),\n",
        "], bbox_params=A.BboxParams(format=\"pascal_voc\",label_fields=[\"class_labels\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg52Z5EDC8VK"
      },
      "outputs": [],
      "source": [
        "class PotholeDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,df,augs=None):\n",
        "        self.df = df\n",
        "        self.augs = augs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        xmin = row.xmin\n",
        "        ymin = row.ymin\n",
        "        xmax = row.xmax\n",
        "        ymax = row.ymax\n",
        "        bbox = [[xmin,ymin,xmax,ymax]]\n",
        "\n",
        "        img_path = row.filename\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.augs:\n",
        "            data = self.augs(image = img, bboxes=bbox, class_labels = [None])\n",
        "            img = data[\"image\"]\n",
        "\n",
        "            bbox = data[\"bboxes\"][0]\n",
        "\n",
        "        img = torch.from_numpy(img).permute(2,0,1) / 255.0\n",
        "        bbox = torch.Tensor(bbox)\n",
        "\n",
        "        return img, bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN_VVvVpC8VK"
      },
      "outputs": [],
      "source": [
        "trainset = PotholeDataset(train_df,train_augs)\n",
        "validset = PotholeDataset(valid_df,valid_augs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WQeP7ufC8VK"
      },
      "outputs": [],
      "source": [
        "len(validset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVv_LDwdC8VL"
      },
      "outputs": [],
      "source": [
        "img, bbox = trainset[5]\n",
        "\n",
        "xmin, ymin, xmax, ymax = bbox\n",
        "\n",
        "pt1 = (int(xmin), int(ymin))\n",
        "pt2 = (int(xmax), int(ymax))\n",
        "\n",
        "bnd_img = cv2.rectangle(img.permute(1, 2, 0).numpy(),pt1, pt2,(255,0,0),2)\n",
        "plt.imshow(bnd_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74iKREzzC8VL"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,shuffle=True)\n",
        "validLoader = torch.utils.data.DataLoader(validset,batch_size=BATCH_SIZE,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvEl3ndhC8VL"
      },
      "outputs": [],
      "source": [
        "print(\"Total no. batches in trainloader : {}\".format(len(trainloader)))\n",
        "print(\"Total no. batches in validloader : {}\".format(len(validLoader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv-JG6gTC8VM"
      },
      "outputs": [],
      "source": [
        "for images,bboxes in trainloader:\n",
        "    break\n",
        "\n",
        "\n",
        "print(\"Shape of one batch images : {}\".format(images.shape))\n",
        "print(\"Shape of one batch bboxes : {}\".format(bboxes.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7y2dP8fC8VM"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torchvision\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06a1XDGSC8VN"
      },
      "outputs": [],
      "source": [
        "class PotholeModel(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(PotholeModel,self).__init__()\n",
        "\n",
        "        self.backbone = timm.create_model(MODEL_NAME,pretrained=True,num_classes=4)\n",
        "\n",
        "\n",
        "    def forward(self,images,gt_bboxes=None):\n",
        "        predBboxes = self.backbone(images)\n",
        "\n",
        "        if gt_bboxes != None:\n",
        "            loss1 = torchvision.ops.complete_box_iou_loss(predBboxes,gt_bboxes,reduction=\"sum\")\n",
        "            loss2 = nn.functional.smooth_l1_loss(predBboxes,gt_bboxes)\n",
        "            return predBboxes,loss2 + loss1\n",
        "\n",
        "        return predBboxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0gLlqZIC8VN"
      },
      "outputs": [],
      "source": [
        "model = PotholeModel()\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjofd7E7C8VO"
      },
      "outputs": [],
      "source": [
        "random_img = torch.rand(1,3,140,140).to(DEVICE)\n",
        "model(random_img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6VcII4mC8VO"
      },
      "outputs": [],
      "source": [
        "def train(model,dataLoader,optimizer):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for data in tqdm(dataLoader):\n",
        "        images,gt_bboxes = data\n",
        "        images,gt_bboxes = images.to(DEVICE), gt_bboxes.to(DEVICE)\n",
        "\n",
        "        bboxes, loss = model(images,gt_bboxes)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opav_HhBC8VP"
      },
      "outputs": [],
      "source": [
        "def eval(model,dataLoader):\n",
        "    total_loss = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for data in tqdm(dataLoader):\n",
        "            images,gt_bboxes = data\n",
        "            images,gt_bboxes = images.to(DEVICE), gt_bboxes.to(DEVICE)\n",
        "\n",
        "            bboxes, loss = model(images,gt_bboxes)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        return total_loss / len(dataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UObLFeWNC8VP"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHIaOTpY7WdN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1fmXwliC8VQ"
      },
      "outputs": [],
      "source": [
        "best_valid_loss = np.inf\n",
        "EPOCHS = 100  # Set number of epochs to 20\n",
        "# model_path = \"/content/drive/MyDrive/best_model.pt\"\n",
        "# model_path = \"/content/drive/MyDrive/best_model_30.pt\"\n",
        "model_path = \"/content/drive/MyDrive/best_model_singlepothole.pt\"\n",
        "# for i in range(EPOCHS):\n",
        "#     train_loss = train(model, trainloader, optimizer)\n",
        "#     valid_loss = eval(model, validLoader)\n",
        "\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         torch.save(model.state_dict(), model_path)\n",
        "#         print(\"WEIGHTS SAVED!!\")\n",
        "#         best_valid_loss = valid_loss\n",
        "\n",
        "#     print(f\"Epoch : {i + 1}, train Loss : {train_loss:.4f}, valid loss : {valid_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjpnYMlGiHz5"
      },
      "outputs": [],
      "source": [
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), \"pothole_model_checkpoint.pt\")\n",
        "\n",
        "# Load the saved checkpoint\n",
        "#model = PotholeModel()  # Create a new instance of your model\n",
        "#model.load_state_dict(torch.load(\"pothole_model_checkpoint.pt\"))\n",
        "#model.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYgx21CeiVND"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1rvCRqBC8VQ"
      },
      "outputs": [],
      "source": [
        "# Function to calculate perceived focal length\n",
        "def calculate_perceived_focal_length(bbox):\n",
        "    length = bbox[3] - bbox[1]\n",
        "    width = bbox[2] - bbox[0]\n",
        "    pixel_length = length  # Assuming length represents pixel length\n",
        "    camera_distance = 90  # Fixed camera distance in centimeters\n",
        "    return (pixel_length * camera_distance) / width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MvSmZkMC8VR"
      },
      "outputs": [],
      "source": [
        "# Dimension Estimation\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "# Dimension estimation\n",
        "perceived_focal_lengths = []\n",
        "\n",
        "for images, gt_bboxes in validLoader:\n",
        "    images = images.to(DEVICE)\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        pred_bboxes = model(images)\n",
        "    # Calculate perceived focal length for each detected pothole\n",
        "    for pred_bbox in pred_bboxes:\n",
        "        perceived_focal_length = calculate_perceived_focal_length(pred_bbox)\n",
        "        perceived_focal_lengths.append(perceived_focal_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luSbd_YYC8VR"
      },
      "outputs": [],
      "source": [
        "# Calculate average perceived focal length\n",
        "average_perceived_focal_length = torch.mean(torch.tensor(perceived_focal_lengths).cpu()).item()\n",
        "print(f\"Average Perceived Focal Length: {average_perceived_focal_length} cm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPcP2kFGC8VS"
      },
      "outputs": [],
      "source": [
        "def compare_plots(image, gt_bbox, out_bbox):\n",
        "\n",
        "    xmin, ymin, xmax, ymax = gt_bbox\n",
        "\n",
        "    pt1 = (int(xmin), int(ymin))\n",
        "    pt2 = (int(xmax), int(ymax))\n",
        "\n",
        "    out_xmin, out_ymin, out_xmax, out_ymax = out_bbox[0]\n",
        "\n",
        "    out_pt1 = (int(out_xmin), int(out_ymin))\n",
        "    out_pt2 = (int(out_xmax), int(out_ymax))\n",
        "\n",
        "    out_img = cv2.rectangle(image.squeeze().permute(1, 2, 0).cpu().numpy(),pt1, pt2,(0,255,0),2)\n",
        "    out_img = cv2.rectangle(out_img, out_pt1, out_pt2,(255,0,0),2)\n",
        "    out_img = cv2.putText(out_img, 'Ground truth', (int(xmin), int(ymin-5)), cv2.FONT_HERSHEY_COMPLEX, 0.3, (0, 255, 0), 1)\n",
        "    out_img = cv2.putText(out_img, 'Predicted', (int(out_xmin), int(out_ymax+10)), cv2.FONT_HERSHEY_COMPLEX, 0.3, (255, 0, 0), 1)\n",
        "    plt.imshow(out_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDNfjvPBC8VS"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# Function to calculate dimension estimation\n",
        "def calculate_dimensions(bbox):\n",
        "    # Extract bounding box coordinates\n",
        "    xmin, ymin, xmax, ymax = bbox.squeeze().tolist()\n",
        "\n",
        "    # Calculate length and width of the bounding box\n",
        "    length = ymax - ymin\n",
        "    width = xmax - xmin\n",
        "\n",
        "    # Calculate area of the pothole\n",
        "    area = length * width\n",
        "\n",
        "    # Estimate actual dimensions using the perceived focal length\n",
        "    actual_length = (perceived_focal_length * length) / IMG_SIZE  # Assuming image size is used for scaling\n",
        "    actual_width = (perceived_focal_length * width) / IMG_SIZE\n",
        "\n",
        "    return actual_length, actual_width, area\n",
        "\n",
        "# Function to compare plots with dimension estimation\n",
        "def compare_plots_with_dimension(image, gt_bbox, out_bbox):\n",
        "    # Perform dimension estimation\n",
        "    actual_length, actual_width, area = calculate_dimensions(out_bbox)\n",
        "\n",
        "    # Plot the image with bounding boxes and dimension estimation\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
        "\n",
        "    # Plot ground truth bounding box\n",
        "    gt_xmin, gt_ymin, gt_xmax, gt_ymax = gt_bbox\n",
        "    gt_length = gt_ymax - gt_ymin\n",
        "    gt_width = gt_xmax - gt_xmin\n",
        "    gt_area = gt_length * gt_width\n",
        "    ax.add_patch(plt.Rectangle((gt_xmin, gt_ymin), gt_width, gt_length, edgecolor='g', facecolor='none', linewidth=2))\n",
        "    ax.text(gt_xmin, gt_ymin - 10, f'GT Area: {gt_area:.2f}', color='g', fontsize=10)\n",
        "\n",
        "    # Plot detected bounding box\n",
        "    out_xmin, out_ymin, out_xmax, out_ymax = out_bbox.squeeze().tolist()\n",
        "    out_length = out_ymax - out_ymin\n",
        "    out_width = out_xmax - out_xmin\n",
        "    ax.add_patch(plt.Rectangle((out_xmin, out_ymin), out_width, out_length, edgecolor='r', facecolor='none', linewidth=2))\n",
        "    ax.text(out_xmin, out_ymax + 20, f'Estimated Area: {area:.2f}', color='r', fontsize=10)\n",
        "    ax.text(out_xmin, out_ymax + 40, f'Estimated Length: {actual_length:.2f} cm', color='r', fontsize=10)\n",
        "    ax.text(out_xmin, out_ymax + 60, f'Estimated Width: {actual_width:.2f} cm', color='r', fontsize=10)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Perform inference on a sample image and visualize with dimension estimation\n",
        "image, gt_bbox = validset[5]\n",
        "image = image.unsqueeze(0).to(DEVICE)\n",
        "out_bbox = model(image)\n",
        "\n",
        "compare_plots_with_dimension(image, gt_bbox, out_bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLVfCljPC8VT"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image, gt_bbox = validset[5]\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "def bb_intersection_over_union(boxA, boxB):\n",
        "    # determine the (x, y)-coordinates of the intersection rectangle\n",
        "    xA = max(boxA[0], boxB[0])\n",
        "    yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2])\n",
        "    yB = min(boxA[3], boxB[3])\n",
        "\n",
        "    # compute the area of intersection rectangle\n",
        "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
        "    if interArea == 0:\n",
        "        return 0\n",
        "    # compute the area of both the prediction and ground-truth\n",
        "    # rectangles\n",
        "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
        "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
        "\n",
        "    # compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area\n",
        "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\n",
        "    # return the intersection over union value\n",
        "    return iou\n",
        "\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBJpX1M7C8Vc"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image, gt_bbox = validset[12]\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW_ORz4ZC8Vd"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image, gt_bbox = validset[13]\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6bpj3ztC8Vd"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image, gt_bbox = validset[15]\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYtcwdc2C8Vd"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image, gt_bbox = validset[23]\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hsJgT6jC8Ve"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image, gt_bbox = validset[25]\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BOUprlWRnsy"
      },
      "outputs": [],
      "source": [
        "iou = []\n",
        "gt_bboxes = []\n",
        "out_bboxes = []\n",
        "for i in range(len(validset)):\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    image, gt_bbox = validset[i]\n",
        "    gt_bboxes.append(gt_bbox)\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    out_bbox = model(image)\n",
        "    out_bboxes.append(out_bbox)\n",
        "  bbox_iou = bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "  iou.append(bbox_iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFk751kmSS7d"
      },
      "outputs": [],
      "source": [
        "np.mean(iou)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kODQJFbsMxNH"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8906.jpg\"\n",
        "# test_img = get_file_as_tensor(\"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8883.jpg\")\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(40, 40), (78, 78)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [40, 40, 78, 78]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxomo_jKWZWz"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8907.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(33, 20), (80, 60)], outline='green')\n",
        "test_img_draw.rectangle([(90, 39), (128, 128)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox_1 = [33, 20, 80, 60]\n",
        "    gt_bbox_2 = [90, 39, 128, 128]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox_1,out_bbox)\n",
        "    compare_plots(image,gt_bbox_2,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox_1, out_bbox[0]))\n",
        "print(bb_intersection_over_union(gt_bbox_2, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAeL62i0WzTK"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8908.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(3, 15), (50, 58)], outline='green')\n",
        "test_img_draw.rectangle([(70, 27), (125, 128)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox_1 = [3, 15, 50, 58]\n",
        "    gt_bbox_2 = [70, 27, 125, 128]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox_1,out_bbox)\n",
        "    compare_plots(image,gt_bbox_2,out_bbox)\n",
        "\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "print(bb_intersection_over_union(gt_bbox_1, out_bbox[0]))\n",
        "print(bb_intersection_over_union(gt_bbox_2, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QYxjqO-wvU3"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8910.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(49, 35), (87, 58)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [49, 35, 87, 58]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6xbDMV7xRgY"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8911.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(43, 25), (73, 78)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [43, 25, 73, 78]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73_HQt8Lxoq5"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8912.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(38, 43), (88, 84)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [38, 43, 88, 84]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw_GV-pyyILj"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8913.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(25, 35), (80, 74)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [25, 35, 80, 74]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2w2f4tI6FBU"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8914.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(45, 25), (88, 102)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [45, 25, 88, 102]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxSxt81e9qgp"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8915.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(40, 40), (88, 88)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [40, 40, 88, 88]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP-QjxvZ-A-B"
      },
      "outputs": [],
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8916.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(38, 35), (90, 119)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [38, 35, 90, 119]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8889.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(0, 50), (85, 120)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [0, 50, 85, 120]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "8UsY8pmmZJlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8888.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(0, 30), (70, 110)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [0, 30, 70, 110]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "bSO-lyZNZ1eF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8887.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(0, 20), (97, 85)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [0, 20, 97, 85]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "wzanmR9Ha1_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8884.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(55, 85), (80, 100)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [55, 85, 80, 100]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "mAjS3ggPbOzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8883.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(37, 63), (70, 87)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [37, 63, 70, 87]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "NPBDG0zXbgSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_2373.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(5, 5), (70, 40)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [5, 5, 70, 40]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "c0J-FNhpb_pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_2372.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(40, 20), (120, 90)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [40, 20, 120, 90]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "lSqRziLXcUNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_2371.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(45, 40), (70, 70)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [45, 40, 70, 70]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "yjNVB6Tscs8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_2370.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(18, 30), (95, 110)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [18, 30, 95, 110]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "D6npZkludCUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8892.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(7, 22), (105, 70)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [7, 22, 105, 70]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "Wipzh1ordlbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8893.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(25, 35), (90, 87)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [25, 35, 90, 87]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "wqQ8c4m3jHlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/157-Potholes-Compressed/IMG_8894.jpg\"\n",
        "test_img = Image.open(path)\n",
        "test_img = test_img.resize((128, 128))\n",
        "test_img_draw = ImageDraw.Draw(test_img)\n",
        "test_img_draw.rectangle([(40, 18), (80, 87)], outline='green')\n",
        "plt.imshow(test_img)\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "test_img_tensor = transform(test_img)/255\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    image = test_img_tensor\n",
        "    image = image.unsqueeze(0).to(DEVICE)\n",
        "    gt_bbox = [40, 18, 80, 87]\n",
        "    out_bbox = model(image)\n",
        "    compare_plots(image,gt_bbox,out_bbox)\n",
        "    print(out_bbox)\n",
        "    print(gt_bbox)\n",
        "'[]'\n",
        "print(bb_intersection_over_union(gt_bbox, out_bbox[0]))\n",
        "plt.title(f\"IoU = {bb_intersection_over_union(gt_bbox, out_bbox[0]):.2f}\")"
      ],
      "metadata": {
        "id": "mAMsXTCDjvxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iou = []\n",
        "gt_bboxes = []\n",
        "out_bboxes = []\n",
        "\n",
        "for i in range(len(validset)):\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        image, gt_bbox = validset[i]\n",
        "        gt_bboxes.append(gt_bbox)\n",
        "        image = image.unsqueeze(0).to(DEVICE)\n",
        "        out_bbox = model(image)\n",
        "        out_bboxes.append(out_bbox)\n",
        "    bbox_iou = bb_intersection_over_union(gt_bbox, out_bbox[0])\n",
        "    iou.append(bbox_iou)\n",
        "\n",
        "print(len(iou))\n",
        "print(len(gt_bboxes))\n",
        "print(len(out_bboxes))\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "iou_threshold = 0.2\n",
        "\n",
        "for gt_bbox, pred_bbox_list in zip(gt_bboxes, out_bboxes):\n",
        "    gt_present = gt_bbox is not None and isinstance(gt_bbox, (list, tuple, torch.Tensor)) and len(gt_bbox) > 0\n",
        "    pred_present = pred_bbox_list is not None and len(pred_bbox_list) > 0\n",
        "\n",
        "    if gt_present:\n",
        "        y_true.append(1)\n",
        "        if pred_present:\n",
        "            best_iou = bb_intersection_over_union(gt_bbox, pred_bbox_list[0])\n",
        "            y_pred.append(1 if best_iou >= iou_threshold else 0)\n",
        "        else:\n",
        "            y_pred.append(0)\n",
        "    else:\n",
        "        y_true.append(0)\n",
        "        y_pred.append(1 if pred_present else 0)\n",
        "\n",
        "# metrics\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"Accuracy:  {accuracy:.3f}\")\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Pothole\", \"No Pothole\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Kaggle Dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "acKanbV4VMd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iou_1 = [0.3809, 0.4212, 0.0928,0.402,0.1901,0.4316,0.3625,0.3955,0.4538,0.6556,0.446,0.2694,0.4093,0,0.5819,0.641,0.1092,0.3163,0.37,0.1381,0.6407,0.0548,0,0.2111,0.18,0.2196,0.4219,0.3726,0.2641]\n",
        "iou_threshold = 0.2\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# if iou is 0, it should be classified as false positive\n",
        "# if iou > iou_threshold, classify it as true positive\n",
        "# iou < iou_threshold, classify it as false positive\n",
        "\n",
        "for iou in iou_1:\n",
        "    if iou == 0:\n",
        "        y_true.append(0)\n",
        "        y_pred.append(1)\n",
        "\n",
        "    elif iou > iou_threshold:\n",
        "        y_true.append(1)\n",
        "        y_pred.append(1)\n",
        "    else:\n",
        "        y_true.append(1)\n",
        "        y_pred.append(0)\n",
        "\n",
        "\n",
        "# metrics\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"Accuracy:  {accuracy:.3f}\")\n",
        "\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Pothole\", \"No Pothole\"])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Own Dataset\")\n",
        "plt.show"
      ],
      "metadata": {
        "id": "JC_JU6zkkYjk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 531821,
          "sourceId": 973710,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 702771,
          "sourceId": 1228192,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2717245,
          "sourceId": 4692738,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}